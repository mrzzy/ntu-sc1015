{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "524c0bb7-4f20-4e6a-810c-2dabe1053a3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# NTU SC1015 - Yelp Dataset Preparation\n",
    "[Yelp Dataset](https://www.yelp.com/dataset) is a subset of Yelp's internal data for academic research:\n",
    "\n",
    "![Yelp Dataset ERD](/files/shared_uploads/programnom@gmail.com/yelp_dataset_erd.png)\n",
    "\n",
    "## Objective\n",
    "```\n",
    "80K     Dataset_User_Agreement.pdf\n",
    "114M    yelp_academic_dataset_business.json\n",
    "274M    yelp_academic_dataset_checkin.json\n",
    "5.0G    yelp_academic_dataset_review.json\n",
    "173M    yelp_academic_dataset_tip.json\n",
    "3.2G    yelp_academic_dataset_user.json\n",
    "```\n",
    "\n",
    "Since the data files are too large to be processed manually, we will extract a **chronogical** subset of the Yelp Dataset with Spark for further analysis.\n",
    "Additionally, some data transformations (eg. **denormalisation**) will be performed to simplify later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de25900f-efb2-4b4b-acbe-cfe5b27108ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0cc27d-ae71-4e98-8dc3-44b6b4a3ab56",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Load Dataset\n",
    "Load the Yelp dataset with Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c170fa1-7e9d-4232-b4d9-b9310c802979",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prefix = \"yelp_academic_dataset_\"\n",
    "s3_bucket = \"ntu-sc1015-yelp\"\n",
    "tables = [\n",
    "    \"business\",\n",
    "    \"checkin\",\n",
    "    \"review\",\n",
    "    \"tip\",\n",
    "    \"user\",\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "for table in tables:\n",
    "    dataset[table] = spark.read.json(f\"s3://{s3_bucket}/{prefix}{table}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0bc3080-66c8-4518-a41d-5bff9c67670c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Prefix column names with to prevent name collisions when joining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae78acd-2bbd-40a9-a12f-83852573fceb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table in [\"business\", \"user\"]:\n",
    "    for col in dataset[table].columns:\n",
    "        if col != f\"{table}_id\":\n",
    "            dataset[table] = dataset[table].withColumnRenamed(col, f\"{table}_{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b49be700-9429-442f-9299-63099c245fd4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Presort `Business` & `User` tables to speed up later sort merge joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37989ab1-3506-42f1-8131-17a79274a98c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "business_df = dataset[\"business\"].sort(\"business_id\").cache()\n",
    "user_df = dataset[\"user\"].sort(\"user_id\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f259a04e-ef76-4180-971a-651aa35f815b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reviews\n",
    "Sample last `n_sample` reviews & denormalise review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8c4641-b660-4873-b415-4dfb805157fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_sample = 100000\n",
    "\n",
    "(\n",
    "    dataset[\"review\"].withColumn(\"date\", F.to_date(\"date\"))\n",
    "    # sample the n_sample last reviews in chronological order to ensure that reviews are contiguous over time. \n",
    "    .sort(\"date\", ascending=False)\n",
    "    .limit(n_sample)\n",
    "    # denormalise by joining use and business data to reviews\n",
    "    # join business first as it is the smallest table\n",
    "    .join(business_df, \"business_id\")\n",
    "    .join(user_df, \"user_id\")\n",
    "    # write as one parquet file back to s3\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(f\"s3a://{s3_bucket}/yelp_reviews.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06337c60-aabc-4aa0-ba3c-cb05df34c670",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Tips\n",
    "Sample last `n_sample` tips & denormalise Tips data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "326f7cf1-2bd7-46c2-ace1-7eb30cf8ee2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    dataset[\"tip\"].withColumn(\"date\", F.to_date(\"date\"))\n",
    "    # sample the n_sample last tips in chronological order to ensure that tips are contiguous over time. \n",
    "    .sort(\"date\", ascending=False)\n",
    "    .limit(n_sample)\n",
    "    # denormalise by joining use and business data to tips\n",
    "    # join business first as it is the smallest table\n",
    "    .join(business_df, \"business_id\")\n",
    "    .join(user_df, \"user_id\")\n",
    "    # write as one parquet file back to s3\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(f\"s3a://{s3_bucket}/yelp_tips.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99f2dbf-1cc1-4104-86c0-a5aa5a1d0f1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Check Ins\n",
    "Check ins are represented as a list of comma-seperated timestamps in the dataset.\n",
    "- Expand comma seperated list into individual check in rows.\n",
    "- Denormalise check ins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88377c94-96d6-4e36-b302-d1a57ca19311",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    dataset[\"checkin\"].join(business_df, \"business_id\")\n",
    "    # expand comma delimited dates into individual rows\n",
    "    .select(F.explode(F.split(\"date\", \",\")))\n",
    "    # write as one parquet file back to s3\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(f\"s3a://{s3_bucket}/yelp_checkins.parquet\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NTU SC1015 - Yelp Dataset Preparation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
